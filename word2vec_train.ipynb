{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.preprocessing import preprocess_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"medium_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mental Note Vol. 24</td>\n",
       "      <td>Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your Brain On Coronavirus</td>\n",
       "      <td>Your Brain On Coronavirus\\n\\nA guide to the cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mind Your Nose</td>\n",
       "      <td>Mind Your Nose\\n\\nHow smell training can chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 4 Purposes of Dreams</td>\n",
       "      <td>Passionate about the synergy between science a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surviving a Rod Through the Head</td>\n",
       "      <td>You’ve heard of him, haven’t you? Phineas Gage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192363</th>\n",
       "      <td>Why do you need a cleaning service?</td>\n",
       "      <td>What could be more important than having a tid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192364</th>\n",
       "      <td>Daily cleaning and maintenance of bedding</td>\n",
       "      <td>Daily cleaning and maintenance of bedding\\n\\nW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192365</th>\n",
       "      <td>Beneficial Advice on Bond Cleaning!</td>\n",
       "      <td>The most important chore at the end is bond cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192366</th>\n",
       "      <td>How I Learned Romanian in 37 Easy Steps</td>\n",
       "      <td>How I Learned Romanian in 37 Easy Steps\\n\\nHey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192367</th>\n",
       "      <td>Trying Pimsleur Cantonese in Hong Kong</td>\n",
       "      <td>Over the past few years, I’ve heard a number o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192368 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title  \\\n",
       "0                             Mental Note Vol. 24   \n",
       "1                       Your Brain On Coronavirus   \n",
       "2                                  Mind Your Nose   \n",
       "3                        The 4 Purposes of Dreams   \n",
       "4                Surviving a Rod Through the Head   \n",
       "...                                           ...   \n",
       "192363        Why do you need a cleaning service?   \n",
       "192364  Daily cleaning and maintenance of bedding   \n",
       "192365        Beneficial Advice on Bond Cleaning!   \n",
       "192366    How I Learned Romanian in 37 Easy Steps   \n",
       "192367     Trying Pimsleur Cantonese in Hong Kong   \n",
       "\n",
       "                                                     text  \n",
       "0       Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...  \n",
       "1       Your Brain On Coronavirus\\n\\nA guide to the cu...  \n",
       "2       Mind Your Nose\\n\\nHow smell training can chang...  \n",
       "3       Passionate about the synergy between science a...  \n",
       "4       You’ve heard of him, haven’t you? Phineas Gage...  \n",
       "...                                                   ...  \n",
       "192363  What could be more important than having a tid...  \n",
       "192364  Daily cleaning and maintenance of bedding\\n\\nW...  \n",
       "192365  The most important chore at the end is bond cl...  \n",
       "192366  How I Learned Romanian in 37 Easy Steps\\n\\nHey...  \n",
       "192367  Over the past few years, I’ve heard a number o...  \n",
       "\n",
       "[192368 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Tokenize each sentence into words and preprocess\n",
    "    return [preprocess_string(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               Mental Note Vol. 24\n",
       "1                         Your Brain On Coronavirus\n",
       "2                                    Mind Your Nose\n",
       "3                          The 4 Purposes of Dreams\n",
       "4                  Surviving a Rod Through the Head\n",
       "                            ...                    \n",
       "192363          Why do you need a cleaning service?\n",
       "192364    Daily cleaning and maintenance of bedding\n",
       "192365          Beneficial Advice on Bond Cleaning!\n",
       "192366      How I Learned Romanian in 37 Easy Steps\n",
       "192367       Trying Pimsleur Cantonese in Hong Kong\n",
       "Name: title, Length: 192368, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "df = df[['title', 'text']]\n",
    "\n",
    "# Preprocess text data\n",
    "df['title'] = df['title'].astype(str)\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['title'] = df['title'].apply(preprocess_string)\n",
    "df['text'] = df['text'].apply(preprocess_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[mental, note, vol]</td>\n",
       "      <td>[photo, josh, riemer, unsplash, merri, christm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[brain, coronaviru]</td>\n",
       "      <td>[brain, coronaviru, guid, curiou, troubl, impa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[mind, nose]</td>\n",
       "      <td>[mind, nose, smell, train, chang, brain, week,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[purpos, dream]</td>\n",
       "      <td>[passion, synergi, scienc, technolog, provid, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[surviv, rod, head]</td>\n",
       "      <td>[you’v, heard, haven’t, phinea, gage, railroad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192363</th>\n",
       "      <td>[need, clean, servic]</td>\n",
       "      <td>[import, have, tidi, organ, home, work, famili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192364</th>\n",
       "      <td>[daili, clean, mainten, bed]</td>\n",
       "      <td>[daili, clean, mainten, bed, clean, product, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192365</th>\n",
       "      <td>[benefici, advic, bond, clean]</td>\n",
       "      <td>[import, chore, end, bond, clean, tenant, cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192366</th>\n",
       "      <td>[learn, romanian, easi, step]</td>\n",
       "      <td>[learn, romanian, easi, step, hei, actual, rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192367</th>\n",
       "      <td>[try, pimsleur, cantones, hong, kong]</td>\n",
       "      <td>[past, year, i’v, heard, number, posit, review...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192368 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title  \\\n",
       "0                         [mental, note, vol]   \n",
       "1                         [brain, coronaviru]   \n",
       "2                                [mind, nose]   \n",
       "3                             [purpos, dream]   \n",
       "4                         [surviv, rod, head]   \n",
       "...                                       ...   \n",
       "192363                  [need, clean, servic]   \n",
       "192364           [daili, clean, mainten, bed]   \n",
       "192365         [benefici, advic, bond, clean]   \n",
       "192366          [learn, romanian, easi, step]   \n",
       "192367  [try, pimsleur, cantones, hong, kong]   \n",
       "\n",
       "                                                     text  \n",
       "0       [photo, josh, riemer, unsplash, merri, christm...  \n",
       "1       [brain, coronaviru, guid, curiou, troubl, impa...  \n",
       "2       [mind, nose, smell, train, chang, brain, week,...  \n",
       "3       [passion, synergi, scienc, technolog, provid, ...  \n",
       "4       [you’v, heard, haven’t, phinea, gage, railroad...  \n",
       "...                                                   ...  \n",
       "192363  [import, have, tidi, organ, home, work, famili...  \n",
       "192364  [daili, clean, mainten, bed, clean, product, d...  \n",
       "192365  [import, chore, end, bond, clean, tenant, cont...  \n",
       "192366  [learn, romanian, easi, step, hei, actual, rea...  \n",
       "192367  [past, year, i’v, heard, number, posit, review...  \n",
       "\n",
       "[192368 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Tokenize each sentence into words and preprocess\n",
    "    return [preprocess_string(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_word2vec(df):\n",
    "    sentences = []\n",
    "    for row in df:\n",
    "        sentences.extend(preprocess_text(str(row)))\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Word2Vec\n",
    "sentences = prepare_data_for_word2vec(df['text'])\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = train_word2vec_model(sentences)\n",
    "\n",
    "# Save the model or perform further analysis\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_model(sentences, vector_size=100, window=5, min_count=1, workers=4):\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"medium_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"medium_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['mental', 'note', 'vol']</td>\n",
       "      <td>['photo', 'josh', 'riemer', 'unsplash', 'merri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['brain', 'coronaviru']</td>\n",
       "      <td>['brain', 'coronaviru', 'guid', 'curiou', 'tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['mind', 'nose']</td>\n",
       "      <td>['mind', 'nose', 'smell', 'train', 'chang', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['purpos', 'dream']</td>\n",
       "      <td>['passion', 'synergi', 'scienc', 'technolog', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['surviv', 'rod', 'head']</td>\n",
       "      <td>['you’v', 'heard', 'haven’t', 'phinea', 'gage'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192363</th>\n",
       "      <td>['need', 'clean', 'servic']</td>\n",
       "      <td>['import', 'have', 'tidi', 'organ', 'home', 'w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192364</th>\n",
       "      <td>['daili', 'clean', 'mainten', 'bed']</td>\n",
       "      <td>['daili', 'clean', 'mainten', 'bed', 'clean', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192365</th>\n",
       "      <td>['benefici', 'advic', 'bond', 'clean']</td>\n",
       "      <td>['import', 'chore', 'end', 'bond', 'clean', 't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192366</th>\n",
       "      <td>['learn', 'romanian', 'easi', 'step']</td>\n",
       "      <td>['learn', 'romanian', 'easi', 'step', 'hei', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192367</th>\n",
       "      <td>['try', 'pimsleur', 'cantones', 'hong', 'kong']</td>\n",
       "      <td>['past', 'year', 'i’v', 'heard', 'number', 'po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192368 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                             ['mental', 'note', 'vol']   \n",
       "1                               ['brain', 'coronaviru']   \n",
       "2                                      ['mind', 'nose']   \n",
       "3                                   ['purpos', 'dream']   \n",
       "4                             ['surviv', 'rod', 'head']   \n",
       "...                                                 ...   \n",
       "192363                      ['need', 'clean', 'servic']   \n",
       "192364             ['daili', 'clean', 'mainten', 'bed']   \n",
       "192365           ['benefici', 'advic', 'bond', 'clean']   \n",
       "192366            ['learn', 'romanian', 'easi', 'step']   \n",
       "192367  ['try', 'pimsleur', 'cantones', 'hong', 'kong']   \n",
       "\n",
       "                                                     text  \n",
       "0       ['photo', 'josh', 'riemer', 'unsplash', 'merri...  \n",
       "1       ['brain', 'coronaviru', 'guid', 'curiou', 'tro...  \n",
       "2       ['mind', 'nose', 'smell', 'train', 'chang', 'b...  \n",
       "3       ['passion', 'synergi', 'scienc', 'technolog', ...  \n",
       "4       ['you’v', 'heard', 'haven’t', 'phinea', 'gage'...  \n",
       "...                                                   ...  \n",
       "192363  ['import', 'have', 'tidi', 'organ', 'home', 'w...  \n",
       "192364  ['daili', 'clean', 'mainten', 'bed', 'clean', ...  \n",
       "192365  ['import', 'chore', 'end', 'bond', 'clean', 't...  \n",
       "192366  ['learn', 'romanian', 'easi', 'step', 'hei', '...  \n",
       "192367  ['past', 'year', 'i’v', 'heard', 'number', 'po...  \n",
       "\n",
       "[192368 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec_model(sentences, vector_size=100, window=5, min_count=1, workers=4):\n",
    "    model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_word2vec_model(df['text'], workers=-1)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import spacy\n",
    "from evaluation import evaluate_clusters\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "def predict_paragraphs(new_text, word2vec_model, dbscan_model):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Tokenize sentences\n",
    "    new_sentences = [sent.text for sent in nlp(new_text).sents]\n",
    "    \n",
    "    # Word2Vec Vectorization\n",
    "    sentence_vectors = []\n",
    "    for sentence in new_sentences:\n",
    "        word_vectors = [] \n",
    "        for word in sentence.split():\n",
    "            if word in word2vec_model.wv:\n",
    "                word_vectors.append(word2vec_model.wv[word])\n",
    "        if len(word_vectors) > 0:\n",
    "            sentence_vector = np.mean(word_vectors, axis=0)\n",
    "            sentence_vectors.append(sentence_vector)\n",
    "            \n",
    "    new_X = np.array(sentence_vectors)\n",
    "    \n",
    "    # DBSCAN prediction\n",
    "    new_clusters = fcluster(linkage(new_X), t=1.5) \n",
    "\n",
    "    \n",
    "    # Map sentences to clusters\n",
    "    new_sentence_clusters = {}\n",
    "    for i, cluster_label in enumerate(new_clusters):\n",
    "        if cluster_label not in new_sentence_clusters:\n",
    "            new_sentence_clusters[cluster_label] = []\n",
    "        new_sentence_clusters[cluster_label].append(new_sentences[i])\n",
    "\n",
    "    # Extract paragraphs \n",
    "    new_paragraphs = [\" \".join(cluster) for cluster in new_sentence_clusters.values()]  \n",
    "    #silhouette_avg = evaluate_clusters(new_X, new_clusters)\n",
    "    \n",
    "    #print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "    \n",
    "    return new_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_folder\n",
    "from config import UPLOAD_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_models\n",
    "dbscan, _ = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.cluster._dbscan.DBSCAN"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = process_folder(UPLOAD_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = []\n",
    "\n",
    "for _, txt in extracted_text.items():\n",
    "    txts.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \" \".join(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = predict_paragraphs(full_text, model, dbscan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Markov Chains\\nIntroduction\\nMarkov chains and Monte Carlo Markov chains are mathematical processes that are based on probability and are commonly used in statistics. A Markov chain is a random process that moves from one state to another, with the next state depending only on the current state - hence it has the Markov \"memoryless\" property. A simple example is modeling a sequence of days with sunny or rainy weather, where the weather on a given day is randomly determined based solely on the weather of the previous day. Monte Carlo Markov chains are a class of algorithms that rely on repeated random sampling to obtain numerical results, leveraging the Markov chain framework to model systems like weather, stock prices, or molecular interactions. A key application is statistical inference - using Monte Carlo simulation of a Markov process to estimate properties of complex distributions and models that cannot be easily analyzed directly. Overall, Markov chains provide a mathematical framework to model random processes over time, while Monte Carlo Markov chains use this to build simulation models for generating sample data from probability distributions of interest across many areas of science, finance, and machine learning.\\nFormulating Markov Chains Mathematically\\nAt its core, a Markov chain comprises a discrete set of states and transition probabilities between each pair of states. This is commonly represented as a state transition matrix , where each element  denotes the probability of moving from state  to state  on a given step. The current state combined with  fully parametrizes the next state - encapsulating the memoryless Markov property. Mathematically, if  is a random variable representing the system state at time , then \\n\\n This formula signifies the conditional distribution of future states depends only on the current state. Based on P, foundational Markov chain analysis provides equilibria, stationary distributions, ergodic behaviors, and other statistical properties. Algorithms like Monte Carlo methods can also simulate trajectories of Markov processes by randomly hopping states guided by the transition probabilities. This pure probabilistic approach enables modeling phenomena from physics, biology, and beyond for theoretical and practical insights. \\n\\nText Generation through Markov Processes\\nText generation can be formulated as a sequential decision-making problem well-suited for Markov decision processes (MDPs). The Markov property - where the current state encapsulates necessary context - allows generating text word-by-word based on preceding terms. This establishes a lightweight, yet versatile approach compared to heavy parameterization with neural networks.\\nConsider a Markov chain text model that gives higher rewards for coherent sentences conforming to grammar and lower rewards otherwise. By optimizing cumulative future reward, the model learns probable transitions between words reflecting sensible narratives. Such statistical text generation circumvents manual rule-encoding.\\nFor example, initializing the state with \"Alice was\" and sampling subsequent actions as words yield:\\n\"Alice was heading to the store when...\"\\n\"Alice was shocked to discover that...\"\\n\"Alice was running late for her appointment...\"\\nThe model associates those sentences starting with the phrase \"Alice was\" which commonly transitions towards other verbs or plot-advancing events.\\nMarkov decision processes apply such iterative reward feedback grounded in state transitions to optimize text generation policies. Their efficient yet effective learning drives adoptions in chatbots for dialog modeling or summarization systems for extracting key points - producing sensible language output without laborious feature engineering. By rewarding linguistic coherency, Markovian reinforcement learning promises to advance automated and creative text generation. \\nMonte Carlo Method\\nThe Monte Carlo method refers to a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The basic concept is to use probability statistics computed from simulations using random number sequences to estimate properties of some process or model. A very simple example is estimating  - we could generate random (x,y) points on a square enclosed in a circle and count what fraction falls inside the circle to estimate its area relative to the square. As the number of samples increases, the estimate converges to , based on area geometry. In computer science, Monte Carlo methods are commonly used for risk analysis, optimization, inference, and machine learning. Common applications include pathfinding, decision tree learning, and evaluating multidimensional integrals in physics simulations that are too complex to solve analytically. The algorithms are useful since they are often easier to implement than deriving explicit solutions while providing great flexibility. With advances in computational power, Monte Carlo is a versatile, parallelizable, and ever-growing approach for practical stochastic modeling and estimation.\\nMonte Carlo Markov Chains (MCMC)\\nA Monte Carlo Markov chain (MCMC) combines Markov chain sampling with Monte Carlo simulation for efficient numerical analysis and statistical estimation. The key advantage of a Markov chain process is that future states depend only on the current state - no historical trajectory is required. This memoryless property allows hopping to completely new states based solely on transition probabilities. When combined with random sampling over many independent iterations in the Monte Carlo framework, MCMC allows efficient exploring and learning of the properties of extremely complex high-dimensional probability distributions, used widely from computational physics to modern machine learning. The states traversed form a Markov chain, while the randomness injected through Monte Carlo testing enables broader coverage for improved generalizability. Together they provide a versatile yet lightweight framework to model real-world stochastic processes like financial trends, genome sequencing, and social networks. The Markov assumption reduces complexity for model learning, while simulation-based inference maps complex spaces difficult to be studied analytically. Their synergy thereby expands the scope and scalability for statistical analysis.\\nMCMC in layman\\'s terms\\nMCMC is like playing a board game to explore new places. Imagine you are playing Snakes and Ladders. Your piece starts at the beginning. Where you land next depends only on your current spot - if you land at the bottom of a ladder, you climb up and advance faster! This is like a Markov Chain, where you hop directly between connected spots based on set rules.\\nNow let us add dice rolling like in Monopoly. On each turn, you advance a random number of steps based on the rolled dice value. This randomness helps you explore more spots, not only the connected ones. Even far-away spots now have a chance of being visited through random big dice rolls combined with climbing ladders. This way you get to know the board better compared to moving along just adjacent squares!\\nMCMC stats methods do the same - randomized Monte Carlo dice rolls are combined with Markov Chain ladder climbs to solve problems. It allows studying huge complex game boards by directly hopping between interesting features through many simulated traversals, something too long by standard play. The random dice rolls ensure you experience more possibilities to learn the rules better!\\nIn summary, MCMC combines targeted but stable Markov Chain exploration with excited dashing about via Monte Carlo randomness - letting you understand big new worlds faster.\\nMCMC in Robots\\nMonte Carlo Markov chains demonstrate great utility for reinforcement learning algorithms seeking to maximize cumulative rewards by interacting with complex environments. Consider training a robot to navigate obstacle courses. The state space encoding positions and sensor data are enormous - far too large to explore exhaustively in a reasonable time. Instead, a Markov chain model is created to hop between proximate states based on a learned policy, avoiding obstacles while progressing toward goal locations that yield rewards. By incorporating randomness to regularly sample exploratory actions during training, the Monte Carlo aspect ensures better coverage for improved policy learning. The robot simulates experience by traversing the Markov chain policy, integrating rewards over time to refine decisions. After sufficient simulation iterations, high-reward state-action sequences are discovered without prohibitively expensive physical trial-and-error. The emergency policy maps large state spaces to productive actions. Blending temporary memoryless state transitions with randomized jumps thus makes Monte Carlo Markov methods exceptionally suitable for reinforcement learning problems.\\n\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "def segment_documents_into_paragraphs(document_list, eps=0.5, min_samples=2, min_paragraph_size=3, max_paragraph_size=10):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    paragraphs = []\n",
    "\n",
    "    for document in document_list:\n",
    "        # Split document into sentences\n",
    "        sentences = nltk.sent_tokenize(document)\n",
    "\n",
    "        # Generate embeddings for each sentence\n",
    "        embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "        # Apply DBSCAN clustering\n",
    "        clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine').fit(embeddings.cpu())\n",
    "        labels = clustering.labels_\n",
    "\n",
    "        # Initial grouping of sentences into paragraphs\n",
    "        temp_paragraphs = []\n",
    "        paragraph = []\n",
    "        for sentence, label in zip(sentences, labels):\n",
    "            if label == -1 or len(paragraph) >= max_paragraph_size:\n",
    "                if paragraph:\n",
    "                    temp_paragraphs.append(paragraph)\n",
    "                    paragraph = []\n",
    "                temp_paragraphs.append([sentence])\n",
    "            else:\n",
    "                paragraph.append(sentence)\n",
    "\n",
    "        if paragraph:\n",
    "            temp_paragraphs.append(paragraph)\n",
    "\n",
    "        # Post-processing: Adjusting paragraph sizes\n",
    "        for paragraph in temp_paragraphs:\n",
    "            if len(paragraph) < min_paragraph_size:\n",
    "                if paragraphs:\n",
    "                    paragraphs[-1].extend(paragraph)\n",
    "                else:\n",
    "                    paragraphs.append(paragraph)\n",
    "            else:\n",
    "                paragraphs.append(paragraph)\n",
    "\n",
    "        # Splitting large paragraphs\n",
    "        final_paragraphs = []\n",
    "        for paragraph in paragraphs:\n",
    "            if len(paragraph) > max_paragraph_size:\n",
    "                for i in range(0, len(paragraph), max_paragraph_size):\n",
    "                    final_paragraphs.append(paragraph[i:i + max_paragraph_size])\n",
    "            else:\n",
    "                final_paragraphs.append(paragraph)\n",
    "\n",
    "    # Convert list of sentences to text paragraphs\n",
    "    paragraphs_text = [' '.join(p) for p in final_paragraphs]\n",
    "\n",
    "    return paragraphs_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = segment_documents_into_paragraphs(txts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Markov Chains\\nIntroduction\\nMarkov chains and Monte Carlo Markov chains are mathematical processes that are based on probability and are commonly used in statistics. A Markov chain is a random process that moves from one state to another, with the next state depending only on the current state - hence it has the Markov \"memoryless\" property. A simple example is modeling a sequence of days with sunny or rainy weather, where the weather on a given day is randomly determined based solely on the weather of the previous day. Monte Carlo Markov chains are a class of algorithms that rely on repeated random sampling to obtain numerical results, leveraging the Markov chain framework to model systems like weather, stock prices, or molecular interactions. A key application is statistical inference - using Monte Carlo simulation of a Markov process to estimate properties of complex distributions and models that cannot be easily analyzed directly. Overall, Markov chains provide a mathematical framework to model random processes over time, while Monte Carlo Markov chains use this to build simulation models for generating sample data from probability distributions of interest across many areas of science, finance, and machine learning. Formulating Markov Chains Mathematically\\nAt its core, a Markov chain comprises a discrete set of states and transition probabilities between each pair of states. This is commonly represented as a state transition matrix , where each element  denotes the probability of moving from state  to state  on a given step. The current state combined with  fully parametrizes the next state - encapsulating the memoryless Markov property. Mathematically, if  is a random variable representing the system state at time , then \\n\\n This formula signifies the conditional distribution of future states depends only on the current state.',\n",
       " 'Based on P, foundational Markov chain analysis provides equilibria, stationary distributions, ergodic behaviors, and other statistical properties.',\n",
       " 'Algorithms like Monte Carlo methods can also simulate trajectories of Markov processes by randomly hopping states guided by the transition probabilities. This pure probabilistic approach enables modeling phenomena from physics, biology, and beyond for theoretical and practical insights. Text Generation through Markov Processes\\nText generation can be formulated as a sequential decision-making problem well-suited for Markov decision processes (MDPs). The Markov property - where the current state encapsulates necessary context - allows generating text word-by-word based on preceding terms. This establishes a lightweight, yet versatile approach compared to heavy parameterization with neural networks.',\n",
       " 'Consider a Markov chain text model that gives higher rewards for coherent sentences conforming to grammar and lower rewards otherwise. By optimizing cumulative future reward, the model learns probable transitions between words reflecting sensible narratives. Such statistical text generation circumvents manual rule-encoding. For example, initializing the state with \"Alice was\" and sampling subsequent actions as words yield:\\n\"Alice was heading to the store when...\"\\n\"Alice was shocked to discover that...\"\\n\"Alice was running late for her appointment...\"\\nThe model associates those sentences starting with the phrase \"Alice was\" which commonly transitions towards other verbs or plot-advancing events. Markov decision processes apply such iterative reward feedback grounded in state transitions to optimize text generation policies. Their efficient yet effective learning drives adoptions in chatbots for dialog modeling or summarization systems for extracting key points - producing sensible language output without laborious feature engineering.',\n",
       " 'By rewarding linguistic coherency, Markovian reinforcement learning promises to advance automated and creative text generation. Monte Carlo Method\\nThe Monte Carlo method refers to a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The basic concept is to use probability statistics computed from simulations using random number sequences to estimate properties of some process or model. A very simple example is estimating  - we could generate random (x,y) points on a square enclosed in a circle and count what fraction falls inside the circle to estimate its area relative to the square. As the number of samples increases, the estimate converges to , based on area geometry.',\n",
       " 'In computer science, Monte Carlo methods are commonly used for risk analysis, optimization, inference, and machine learning. Common applications include pathfinding, decision tree learning, and evaluating multidimensional integrals in physics simulations that are too complex to solve analytically. The algorithms are useful since they are often easier to implement than deriving explicit solutions while providing great flexibility. With advances in computational power, Monte Carlo is a versatile, parallelizable, and ever-growing approach for practical stochastic modeling and estimation. Monte Carlo Markov Chains (MCMC)\\nA Monte Carlo Markov chain (MCMC) combines Markov chain sampling with Monte Carlo simulation for efficient numerical analysis and statistical estimation. The key advantage of a Markov chain process is that future states depend only on the current state - no historical trajectory is required. This memoryless property allows hopping to completely new states based solely on transition probabilities. When combined with random sampling over many independent iterations in the Monte Carlo framework, MCMC allows efficient exploring and learning of the properties of extremely complex high-dimensional probability distributions, used widely from computational physics to modern machine learning. The states traversed form a Markov chain, while the randomness injected through Monte Carlo testing enables broader coverage for improved generalizability. Together they provide a versatile yet lightweight framework to model real-world stochastic processes like financial trends, genome sequencing, and social networks.',\n",
       " \"The Markov assumption reduces complexity for model learning, while simulation-based inference maps complex spaces difficult to be studied analytically. Their synergy thereby expands the scope and scalability for statistical analysis. MCMC in layman's terms\\nMCMC is like playing a board game to explore new places. Imagine you are playing Snakes and Ladders. Your piece starts at the beginning.\",\n",
       " 'Where you land next depends only on your current spot - if you land at the bottom of a ladder, you climb up and advance faster! This is like a Markov Chain, where you hop directly between connected spots based on set rules. Now let us add dice rolling like in Monopoly. On each turn, you advance a random number of steps based on the rolled dice value. This randomness helps you explore more spots, not only the connected ones. Even far-away spots now have a chance of being visited through random big dice rolls combined with climbing ladders. This way you get to know the board better compared to moving along just adjacent squares! MCMC stats methods do the same - randomized Monte Carlo dice rolls are combined with Markov Chain ladder climbs to solve problems. It allows studying huge complex game boards by directly hopping between interesting features through many simulated traversals, something too long by standard play. The random dice rolls ensure you experience more possibilities to learn the rules better!',\n",
       " 'In summary, MCMC combines targeted but stable Markov Chain exploration with excited dashing about via Monte Carlo randomness - letting you understand big new worlds faster. MCMC in Robots\\nMonte Carlo Markov chains demonstrate great utility for reinforcement learning algorithms seeking to maximize cumulative rewards by interacting with complex environments. Consider training a robot to navigate obstacle courses. The state space encoding positions and sensor data are enormous - far too large to explore exhaustively in a reasonable time.',\n",
       " 'Instead, a Markov chain model is created to hop between proximate states based on a learned policy, avoiding obstacles while progressing toward goal locations that yield rewards. By incorporating randomness to regularly sample exploratory actions during training, the Monte Carlo aspect ensures better coverage for improved policy learning. The robot simulates experience by traversing the Markov chain policy, integrating rewards over time to refine decisions. After sufficient simulation iterations, high-reward state-action sequences are discovered without prohibitively expensive physical trial-and-error. The emergency policy maps large state spaces to productive actions. Blending temporary memoryless state transitions with randomized jumps thus makes Monte Carlo Markov methods exceptionally suitable for reinforcement learning problems.']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = segment_paragraphs(str(txts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "paras[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "One of the most famous unsupervised algorithms for text segmentation is TextTiling {2}. It's implemented in NLTK in the nltk.tokenize.texttiling module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'list'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict_paragraphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbscan\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mpredict_paragraphs\u001b[0;34m(new_text, word2vec_model, dbscan_model)\u001b[0m\n\u001b[1;32m      7\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Tokenize and preprocess sentences using spaCy\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m new_sentences \u001b[38;5;241m=\u001b[39m [sent\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_text\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msents]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_sentences)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Word2Vec Vectorization using the pre-trained word2vec_model\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/language.py:1002\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    983\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    986\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    987\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1002\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/spacy/language.py:1093\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_doc(doc_like)\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE866\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[0;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'list'>."
     ]
    }
   ],
   "source": [
    "predict_paragraphs(txts, model, dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
