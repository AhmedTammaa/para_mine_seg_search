{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# Folder to store uploaded files \n",
    "\n",
    "\n",
    "# Load models\n",
    "#vectorizer, dbscan_model = load_models()\n",
    "\n",
    "\n",
    "\n",
    "def handle_upload(change):\n",
    "    uploaded_files_path = UPLOAD_FOLDER\n",
    "    \n",
    "    if not os.path.exists(uploaded_files_path):\n",
    "        os.mkdir(uploaded_files_path)\n",
    "    \n",
    "    for file in change['new']:\n",
    "        filename = change['new'][file]['metadata']['name']\n",
    "        unique_filename = str(uuid.uuid4()) + \"-\" + filename\n",
    "        \n",
    "        file_path = os.path.join(uploaded_files_path, unique_filename)\n",
    "       \n",
    "        with open(file_path, \"wb\") as fp:\n",
    "            fp.write(change['new'][file]['content'])\n",
    "            \n",
    "        print(f\"{filename} saved to {file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import utils\n",
    "from predictors import predict_paragraphs, predict_title, segment_documents_into_paragraphs\n",
    "from preprocessing import text_normalization, postprocess_titles\n",
    "from config import UPLOAD_FOLDER\n",
    "class ExtractAndPredict(BaseEstimator, TransformerMixin):\n",
    "\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        text = utils.process_folder(X)\n",
    "        # Concatenate text into one long string\n",
    "        text = \"\".join(text.values())  \n",
    "        #paragraphs = predict_paragraphs(text, self.dbscan, self.vectorizer)\n",
    "        #print(type(text))\n",
    "        paragraphs = segment_documents_into_paragraphs([text], eps=0.5, min_samples=2, min_paragraph_size=3, max_paragraph_size=10)\n",
    "        return paragraphs\n",
    "\n",
    "class CleanParagraphs(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, paragraphs):\n",
    "        #clean = [text_normalization(p) for p in paragraphs] \n",
    "        return paragraphs\n",
    "\n",
    "class TitleGenerator(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer, self.model = utils.load_title_generator()\n",
    "        self.orig_paragraphs = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.orig_paragraphs = X\n",
    "        return self\n",
    "    \n",
    "    def transform(self, clean_paragraphs):\n",
    "        results = []\n",
    "        for idx, para in enumerate(clean_paragraphs):\n",
    "            title = predict_title(para, self.tokenizer, self.model) \n",
    "            title = postprocess_titles(title)\n",
    "            orig = self.orig_paragraphs[idx]\n",
    "            results.append((idx, orig, title))\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('extract', ExtractAndPredict()),\n",
    "    ('clean', CleanParagraphs()),\n",
    "    ('title', TitleGenerator())\n",
    "])\n",
    "\n",
    "paragraph_data = pipe.fit_transform(UPLOAD_FOLDER)\n",
    "#utils.save_paragraphs(paragraph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  'Markov Chains\\nIntroduction\\nMarkov chains and Monte Carlo Markov chains are mathematical processes that are based on probability and are commonly used in statistics. A Markov chain is a random process that moves from one state to another, with the next state depending only on the current state - hence it has the Markov \"memoryless\" property. A simple example is modeling a sequence of days with sunny or rainy weather, where the weather on a given day is randomly determined based solely on the weather of the previous day. Monte Carlo Markov chains are a class of algorithms that rely on repeated random sampling to obtain numerical results, leveraging the Markov chain framework to model systems like weather, stock prices, or molecular interactions. A key application is statistical inference - using Monte Carlo simulation of a Markov process to estimate properties of complex distributions and models that cannot be easily analyzed directly. Overall, Markov chains provide a mathematical framework to model random processes over time, while Monte Carlo Markov chains use this to build simulation models for generating sample data from probability distributions of interest across many areas of science, finance, and machine learning. Formulating Markov Chains Mathematically\\nAt its core, a Markov chain comprises a discrete set of states and transition probabilities between each pair of states. This is commonly represented as a state transition matrix , where each element  denotes the probability of moving from state  to state  on a given step. The current state combined with  fully parametrizes the next state - encapsulating the memoryless Markov property. Mathematically, if  is a random variable representing the system state at time , then \\n\\n This formula signifies the conditional distribution of future states depends only on the current state.',\n",
       "  'Markov Chains Monte Carlo'),\n",
       " (1,\n",
       "  'Based on P, foundational Markov chain analysis provides equilibria, stationary distributions, ergodic behaviors, and other statistical properties.',\n",
       "  'Foundational Markov chain analysis provides equilibria , stationary distributions ergodic behaviors statistical properties .'),\n",
       " (2,\n",
       "  'Algorithms like Monte Carlo methods can also simulate trajectories of Markov processes by randomly hopping states guided by the transition probabilities. This pure probabilistic approach enables modeling phenomena from physics, biology, and beyond for theoretical and practical insights. Text Generation through Markov Processes\\nText generation can be formulated as a sequential decision-making problem well-suited for Markov decision processes (MDPs). The Markov property - where the current state encapsulates necessary context - allows generating text word-by-word based on preceding terms. This establishes a lightweight, yet versatile approach compared to heavy parameterization with neural networks.',\n",
       "  'Text Generation Markov Processes'),\n",
       " (3,\n",
       "  'Consider a Markov chain text model that gives higher rewards for coherent sentences conforming to grammar and lower rewards otherwise. By optimizing cumulative future reward, the model learns probable transitions between words reflecting sensible narratives. Such statistical text generation circumvents manual rule-encoding. For example, initializing the state with \"Alice was\" and sampling subsequent actions as words yield:\\n\"Alice was heading to the store when...\"\\n\"Alice was shocked to discover that...\"\\n\"Alice was running late for her appointment...\"\\nThe model associates those sentences starting with the phrase \"Alice was\" which commonly transitions towards other verbs or plot-advancing events. Markov decision processes apply such iterative reward feedback grounded in state transitions to optimize text generation policies. Their efficient yet effective learning drives adoptions in chatbots for dialog modeling or summarization systems for extracting key points - producing sensible language output without laborious feature engineering.',\n",
       "  'Markov Decision Processes Optimize Text Generation Policies'),\n",
       " (4,\n",
       "  'By rewarding linguistic coherency, Markovian reinforcement learning promises to advance automated and creative text generation. Monte Carlo Method\\nThe Monte Carlo method refers to a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The basic concept is to use probability statistics computed from simulations using random number sequences to estimate properties of some process or model. A very simple example is estimating  - we could generate random (x,y) points on a square enclosed in a circle and count what fraction falls inside the circle to estimate its area relative to the square. As the number of samples increases, the estimate converges to , based on area geometry.',\n",
       "  'Markovian reinforcement learning promises advance automated creative text generation rewarding linguistic coherency ,'),\n",
       " (5,\n",
       "  'In computer science, Monte Carlo methods are commonly used for risk analysis, optimization, inference, and machine learning. Common applications include pathfinding, decision tree learning, and evaluating multidimensional integrals in physics simulations that are too complex to solve analytically. The algorithms are useful since they are often easier to implement than deriving explicit solutions while providing great flexibility. With advances in computational power, Monte Carlo is a versatile, parallelizable, and ever-growing approach for practical stochastic modeling and estimation. Monte Carlo Markov Chains (MCMC)\\nA Monte Carlo Markov chain (MCMC) combines Markov chain sampling with Monte Carlo simulation for efficient numerical analysis and statistical estimation. The key advantage of a Markov chain process is that future states depend only on the current state - no historical trajectory is required. This memoryless property allows hopping to completely new states based solely on transition probabilities. When combined with random sampling over many independent iterations in the Monte Carlo framework, MCMC allows efficient exploring and learning of the properties of extremely complex high-dimensional probability distributions, used widely from computational physics to modern machine learning. The states traversed form a Markov chain, while the randomness injected through Monte Carlo testing enables broader coverage for improved generalizability. Together they provide a versatile yet lightweight framework to model real-world stochastic processes like financial trends, genome sequencing, and social networks.',\n",
       "  'Monte Carlo Markov Chains ( MCMC ): versatile , parallelizable - growing approach practical stochastic modeling estimation'),\n",
       " (6,\n",
       "  \"The Markov assumption reduces complexity for model learning, while simulation-based inference maps complex spaces difficult to be studied analytically. Their synergy thereby expands the scope and scalability for statistical analysis. MCMC in layman's terms\\nMCMC is like playing a board game to explore new places. Imagine you are playing Snakes and Ladders. Your piece starts at the beginning.\",\n",
       "  'Markov assumption reduces complexity model learning , simulation - based inference maps complex spaces difficult studied analytically .'),\n",
       " (7,\n",
       "  'Where you land next depends only on your current spot - if you land at the bottom of a ladder, you climb up and advance faster! This is like a Markov Chain, where you hop directly between connected spots based on set rules. Now let us add dice rolling like in Monopoly. On each turn, you advance a random number of steps based on the rolled dice value. This randomness helps you explore more spots, not only the connected ones. Even far-away spots now have a chance of being visited through random big dice rolls combined with climbing ladders. This way you get to know the board better compared to moving along just adjacent squares! MCMC stats methods do the same - randomized Monte Carlo dice rolls are combined with Markov Chain ladder climbs to solve problems. It allows studying huge complex game boards by directly hopping between interesting features through many simulated traversals, something too long by standard play. The random dice rolls ensure you experience more possibilities to learn the rules better!',\n",
       "  'Monte Carlo Dice Rolls Markov Chain Ladder Climbs'),\n",
       " (8,\n",
       "  'In summary, MCMC combines targeted but stable Markov Chain exploration with excited dashing about via Monte Carlo randomness - letting you understand big new worlds faster. MCMC in Robots\\nMonte Carlo Markov chains demonstrate great utility for reinforcement learning algorithms seeking to maximize cumulative rewards by interacting with complex environments. Consider training a robot to navigate obstacle courses. The state space encoding positions and sensor data are enormous - far too large to explore exhaustively in a reasonable time.',\n",
       "  'MCMC Robots combines targeted stable Markov Chain exploration excited dashing Monte Carlo randomness - letting understand big new worlds faster'),\n",
       " (9,\n",
       "  'Instead, a Markov chain model is created to hop between proximate states based on a learned policy, avoiding obstacles while progressing toward goal locations that yield rewards. By incorporating randomness to regularly sample exploratory actions during training, the Monte Carlo aspect ensures better coverage for improved policy learning. The robot simulates experience by traversing the Markov chain policy, integrating rewards over time to refine decisions. After sufficient simulation iterations, high-reward state-action sequences are discovered without prohibitively expensive physical trial-and-error. The emergency policy maps large state spaces to productive actions. Blending temporary memoryless state transitions with randomized jumps thus makes Monte Carlo Markov methods exceptionally suitable for reinforcement learning problems.',\n",
       "  'Monte Carlo Markov Methods Reinforcement Learning')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph_datah_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(paragraph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "process_para_pipeline = joblib.dump(pipe,\"models/process_para_pipeline.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dominate\n",
    "from dominate.tags import div, p, style, h1\n",
    "\n",
    "def convert_to_html(results):\n",
    "    doc = dominate.document(title='Processed Document')\n",
    "\n",
    "    STYLE = \"\"\"\n",
    "    body {\n",
    "        font-family: 'Georgia', serif;\n",
    "        margin: 0;\n",
    "        padding: 0;\n",
    "        background: #fff;\n",
    "        color: #333;\n",
    "        line-height: 1.6;\n",
    "    }\n",
    "    .container {\n",
    "        width: 80%;\n",
    "        max-width: 800px;\n",
    "        margin: 0 auto;\n",
    "        padding: 20px;\n",
    "    }\n",
    "    h1.title {\n",
    "        text-align: center;\n",
    "        font-size: 28px;\n",
    "        margin-top: 50px;\n",
    "        margin-bottom: 50px;\n",
    "    }\n",
    "    .subtitle {\n",
    "        font-weight: bold;\n",
    "        font-size: 20px;\n",
    "        margin-top: 30px;\n",
    "        margin-bottom: 10px;\n",
    "    }\n",
    "    .paragraph {\n",
    "        font-size: 18px;\n",
    "        margin-bottom: 20px;\n",
    "        text-align: justify;\n",
    "        text-indent: 40px;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    with doc.head:\n",
    "        style(STYLE)\n",
    "\n",
    "    with doc:\n",
    "        with div(cls='container'):\n",
    "            h1('Processed Document', cls='title')\n",
    "            for idx, content, title in results:\n",
    "                with div().add(p(f\"{idx+1}. {title}\", cls='subtitle')):\n",
    "                    p(content, cls='paragraph')\n",
    "\n",
    "    html_file = open('output.html','w')\n",
    "    html_file.write(doc.render())\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dominate.document \"Processed Document\">"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_html(paragraph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def save_csv(paragraphs):\n",
    "    #df = pd.DataFrame(paragraphs, orient='index', columns=['para_content','para_title'])\n",
    "    #df.index.name = 'para_idx'\n",
    "    #df.reset_index(inplace=True)\n",
    "    df.to_csv('gen_para.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(paragraph_data,columns=['para_idx','para_content','para_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para_idx</th>\n",
       "      <th>para_content</th>\n",
       "      <th>para_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Markov Chains\\nIntroduction\\nMarkov chains and...</td>\n",
       "      <td>Markov Chains Monte Carlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A Markov chain is a random process that moves ...</td>\n",
       "      <td>Markov chain random process moves state , depe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A simple example is modeling a sequence of day...</td>\n",
       "      <td>simple example modeling sequence days sunny ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Based on P, foundational Markov chain analysis...</td>\n",
       "      <td>Foundational Markov chain analysis provides eq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n\\nText Generation through Markov Processes\\n...</td>\n",
       "      <td>Text Generation Markov Processes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>The Markov assumption reduces complexity for m...</td>\n",
       "      <td>Markov assumption reduces complexity model lea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   para_idx                                       para_content  \\\n",
       "0         0  Markov Chains\\nIntroduction\\nMarkov chains and...   \n",
       "1         1  A Markov chain is a random process that moves ...   \n",
       "2         2  A simple example is modeling a sequence of day...   \n",
       "3         3  Based on P, foundational Markov chain analysis...   \n",
       "4         4  \\n\\nText Generation through Markov Processes\\n...   \n",
       "5         5  The Markov assumption reduces complexity for m...   \n",
       "\n",
       "                                          para_title  \n",
       "0                          Markov Chains Monte Carlo  \n",
       "1  Markov chain random process moves state , depe...  \n",
       "2  simple example modeling sequence days sunny ra...  \n",
       "3  Foundational Markov chain analysis provides eq...  \n",
       "4                   Text Generation Markov Processes  \n",
       "5  Markov assumption reduces complexity model lea...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gen_para.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "def search_paragraphs(query, paragraphs):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Load Sentence Transformer model\n",
    "    model_name = \"all-mpnet-base-v2\"\n",
    "    sentence_transformer = SentenceTransformer(model_name)\n",
    "\n",
    "    # Extract content from the list of tuples\n",
    "    paragraph_contents = [title+ \" \" + content for idx, content, title in paragraphs.values]\n",
    "\n",
    "    query_embedding = sentence_transformer.encode([query])[0]\n",
    "\n",
    "    # Calculate paragraph embeddings\n",
    "    paragraph_embeddings = sentence_transformer.encode(paragraph_contents)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    scores = util.pytorch_cos_sim(torch.tensor([query_embedding]), torch.tensor(paragraph_embeddings))[0]\n",
    "    scores = scores.cpu().numpy()\n",
    "\n",
    "    # Get the indices of top results\n",
    "    top_indices = scores.argsort()[::-1][:5]\n",
    "\n",
    "    # Create a list of tuples with score, index, and content\n",
    "    top_results = [(scores[idx], idx, paragraphs.iloc[idx][2], paragraphs.iloc[idx][1]) for idx in top_indices]\n",
    "\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search query\n",
    "user_query = \"\"\"A casino\"\"\"\n",
    "\n",
    "# Call the search function\n",
    "result = search_paragraphs(user_query, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nText Generation through Markov Processes\\nText generation can be formulated as a sequential decision-making problem well-suited for Markov decision processes (MDPs). Such statistical text generation circumvents manual rule-encoding. \\nMarkov decision processes apply such iterative reward feedback grounded in state transitions to optimize text generation policies. By rewarding linguistic coherency, Markovian reinforcement learning promises to advance automated and creative text generation.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4]['para_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.12053793,\n",
       "  2,\n",
       "  'simple example modeling sequence days sunny rainy weather , given day randomly determined based solely previous .',\n",
       "  'A simple example is modeling a sequence of days with sunny or rainy weather, where the weather on a given day is randomly determined based solely on the weather of the previous day. This pure probabilistic approach enables modeling phenomena from physics, biology, and beyond for theoretical and practical insights. This establishes a lightweight, yet versatile approach compared to heavy parameterization with neural networks. By optimizing cumulative future reward, the model learns probable transitions between words reflecting sensible narratives. \\nFor example, initializing the state with \"Alice was\" and sampling subsequent actions as words yield:\\n\"Alice was heading to the store when...\"\\n\"Alice was shocked to discover that...\"\\n\"Alice was running late for her appointment...\"\\nThe model associates those sentences starting with the phrase \"Alice was\" which commonly transitions towards other verbs or plot-advancing events. Their efficient yet effective learning drives adoptions in chatbots for dialog modeling or summarization systems for extracting key points - producing sensible language output without laborious feature engineering. A very simple example is estimating  - we could generate random (x,y) points on a square enclosed in a circle and count what fraction falls inside the circle to estimate its area relative to the square. As the number of samples increases, the estimate converges to , based on area geometry. Common applications include pathfinding, decision tree learning, and evaluating multidimensional integrals in physics simulations that are too complex to solve analytically. The algorithms are useful since they are often easier to implement than deriving explicit solutions while providing great flexibility. Together they provide a versatile yet lightweight framework to model real-world stochastic processes like financial trends, genome sequencing, and social networks. \\nMCMC in layman\\'s terms\\nMCMC is like playing a board game to explore new places. Imagine you are playing Snakes and Ladders. Your piece starts at the beginning. Where you land next depends only on your current spot - if you land at the bottom of a ladder, you climb up and advance faster! \\nNow let us add dice rolling like in Monopoly. This randomness helps you explore more spots, not only the connected ones. This way you get to know the board better compared to moving along just adjacent squares! It allows studying huge complex game boards by directly hopping between interesting features through many simulated traversals, something too long by standard play. Consider training a robot to navigate obstacle courses. The state space encoding positions and sensor data are enormous - far too large to explore exhaustively in a reasonable time. After sufficient simulation iterations, high-reward state-action sequences are discovered without prohibitively expensive physical trial-and-error. \\n\\n'),\n",
       " (0.10953793,\n",
       "  5,\n",
       "  'Markov assumption reduces complexity model learning , simulation - based inference maps complex spaces difficult studied analytically .',\n",
       "  'The Markov assumption reduces complexity for model learning, while simulation-based inference maps complex spaces difficult to be studied analytically. The emergency policy maps large state spaces to productive actions.'),\n",
       " (0.107693955,\n",
       "  1,\n",
       "  'Markov chain random process moves state , depending current - \" memoryless property .',\n",
       "  'A Markov chain is a random process that moves from one state to another, with the next state depending only on the current state - hence it has the Markov \"memoryless\" property. This is commonly represented as a state transition matrix , where each element  denotes the probability of moving from state  to state  on a given step. The current state combined with  fully parametrizes the next state - encapsulating the memoryless Markov property. Mathematically, if  is a random variable representing the system state at time , then \\n\\n This formula signifies the conditional distribution of future states depends only on the current state. The Markov property - where the current state encapsulates necessary context - allows generating text word-by-word based on preceding terms. The key advantage of a Markov chain process is that future states depend only on the current state - no historical trajectory is required. This memoryless property allows hopping to completely new states based solely on transition probabilities.'),\n",
       " (0.08651391,\n",
       "  0,\n",
       "  'Markov Chains Monte Carlo',\n",
       "  'Markov Chains\\nIntroduction\\nMarkov chains and Monte Carlo Markov chains are mathematical processes that are based on probability and are commonly used in statistics. Monte Carlo Markov chains are a class of algorithms that rely on repeated random sampling to obtain numerical results, leveraging the Markov chain framework to model systems like weather, stock prices, or molecular interactions. A key application is statistical inference - using Monte Carlo simulation of a Markov process to estimate properties of complex distributions and models that cannot be easily analyzed directly. Overall, Markov chains provide a mathematical framework to model random processes over time, while Monte Carlo Markov chains use this to build simulation models for generating sample data from probability distributions of interest across many areas of science, finance, and machine learning. \\nFormulating Markov Chains Mathematically\\nAt its core, a Markov chain comprises a discrete set of states and transition probabilities between each pair of states. Algorithms like Monte Carlo methods can also simulate trajectories of Markov processes by randomly hopping states guided by the transition probabilities. \\nMonte Carlo Method\\nThe Monte Carlo method refers to a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The basic concept is to use probability statistics computed from simulations using random number sequences to estimate properties of some process or model. In computer science, Monte Carlo methods are commonly used for risk analysis, optimization, inference, and machine learning. With advances in computational power, Monte Carlo is a versatile, parallelizable, and ever-growing approach for practical stochastic modeling and estimation. \\nMonte Carlo Markov Chains (MCMC)\\nA Monte Carlo Markov chain (MCMC) combines Markov chain sampling with Monte Carlo simulation for efficient numerical analysis and statistical estimation. When combined with random sampling over many independent iterations in the Monte Carlo framework, MCMC allows efficient exploring and learning of the properties of extremely complex high-dimensional probability distributions, used widely from computational physics to modern machine learning. The states traversed form a Markov chain, while the randomness injected through Monte Carlo testing enables broader coverage for improved generalizability. On each turn, you advance a random number of steps based on the rolled dice value. Even far-away spots now have a chance of being visited through random big dice rolls combined with climbing ladders. \\nMCMC stats methods do the same - randomized Monte Carlo dice rolls are combined with Markov Chain ladder climbs to solve problems. The random dice rolls ensure you experience more possibilities to learn the rules better! \\nIn summary, MCMC combines targeted but stable Markov Chain exploration with excited dashing about via Monte Carlo randomness - letting you understand big new worlds faster. \\nMCMC in Robots\\nMonte Carlo Markov chains demonstrate great utility for reinforcement learning algorithms seeking to maximize cumulative rewards by interacting with complex environments. By incorporating randomness to regularly sample exploratory actions during training, the Monte Carlo aspect ensures better coverage for improved policy learning. Blending temporary memoryless state transitions with randomized jumps thus makes Monte Carlo Markov methods exceptionally suitable for reinforcement learning problems.'),\n",
       " (0.08319418,\n",
       "  3,\n",
       "  'Foundational Markov chain analysis provides equilibria , stationary distributions ergodic behaviors statistical properties .',\n",
       "  'Based on P, foundational Markov chain analysis provides equilibria, stationary distributions, ergodic behaviors, and other statistical properties. \\nConsider a Markov chain text model that gives higher rewards for coherent sentences conforming to grammar and lower rewards otherwise. Their synergy thereby expands the scope and scalability for statistical analysis. This is like a Markov Chain, where you hop directly between connected spots based on set rules. Instead, a Markov chain model is created to hop between proximate states based on a learned policy, avoiding obstacles while progressing toward goal locations that yield rewards. The robot simulates experience by traversing the Markov chain policy, integrating rewards over time to refine decisions.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "# Use pip freeze to generate requirements from current env\n",
    "result = subprocess.run(['pip', 'freeze'], stdout=subprocess.PIPE)\n",
    "requirements = result.stdout.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'absl-py==1.0.0\\naccelerate==0.25.0\\naiohttp==3.9.1\\naiosignal==1.3.1\\naltair==5.2.0\\nargon2-cffi==21.3.0\\nargon2-cffi-bindings==21.2.0\\nasttokens==2.0.5\\nastunparse==1.6.3\\nasync-timeout==4.0.3\\nattrs==21.4.0\\nbackcall==0.2.0\\nbackports.zoneinfo==0.2.1\\nbeautifulsoup4==4.12.2\\nblack==22.1.0\\nbleach==4.1.0\\nblinker==1.7.0\\nblis==0.7.11\\nbreadability==0.1.20\\ncachetools==5.0.0\\ncatalogue==2.0.10\\ncertifi==2019.11.28\\ncffi==1.15.0\\nchardet==3.0.4\\ncharset-normalizer==3.3.2\\nclick==8.0.3\\ncycler==0.11.0\\ncymem==2.0.8\\ndatasets==2.15.0\\ndbus-python==1.2.16\\ndebugpy==1.5.1\\ndecorator==5.1.1\\ndefusedxml==0.7.1\\ndill==0.3.7\\ndocopt==0.6.2\\ndocx==0.2.4\\ndocx2txt==0.8\\ndominate==2.9.0\\nen-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl\\nentrypoints==0.3\\nexecuting==0.8.2\\nfilelock==3.13.1\\nflatbuffers==2.0\\nfonttools==4.29.1\\nfrozenlist==1.4.1\\nfsspec==2023.10.0\\ngast==0.5.3\\ngensim==4.3.2\\ngitdb==4.0.11\\nGitPython==3.1.40\\ngoogle-auth==2.6.0\\ngoogle-auth-oauthlib==0.4.6\\ngoogle-pasta==0.2.0\\ngrpcio==1.43.0\\nh5py==3.6.0\\nhuggingface-hub==0.19.4\\nidna==2.8\\nimportlib-metadata==4.10.1\\nimportlib-resources==5.13.0\\nipykernel==5.1.1\\nipython==8.0.1\\nipython-genutils==0.2.0\\nipywidgets==7.6.5\\njedi==0.17.2\\nJinja2==3.0.3\\njoblib==1.3.2\\njsonschema==4.4.0\\njupyter==1.0.0\\njupyter-client==7.1.2\\njupyter-console==6.4.0\\njupyter-core==4.9.1\\njupyter-http-over-ws==0.0.8\\njupyterlab-pygments==0.1.2\\njupyterlab-widgets==1.0.2\\nkaggle==1.5.16\\nkeras==2.8.0\\nKeras-Preprocessing==1.1.2\\nkiwisolver==1.3.2\\nlangcodes==3.3.0\\nlibclang==13.0.0\\nlxml==4.9.3\\nMarkdown==3.3.6\\nmarkdown-it-py==3.0.0\\nMarkupSafe==2.0.1\\nmatplotlib==3.4.3\\nmatplotlib-inline==0.1.3\\nmdurl==0.1.2\\nmistune==0.8.4\\nmultidict==6.0.4\\nmultiprocess==0.70.15\\nmurmurhash==1.0.10\\nmypy-extensions==0.4.3\\nnbclient==0.5.10\\nnbconvert==6.4.1\\nnbformat==4.4.0\\nnest-asyncio==1.5.4\\nnetworkx==3.1\\nnltk==3.6.5\\nnotebook==6.4.8\\nnumpy==1.22.1\\noauthlib==3.2.0\\nopt-einsum==3.3.0\\npackaging==21.3\\npandas==1.3.3\\npandocfilters==1.5.0\\nparso==0.7.1\\npathspec==0.9.0\\npathy==0.10.3\\npexpect==4.8.0\\npickleshare==0.7.5\\nPillow==9.0.0\\nplatformdirs==2.4.1\\nplotly==5.18.0\\npreshed==3.0.9\\nprometheus-client==0.13.1\\nprompt-toolkit==3.0.26\\nprotobuf==3.20.0\\npsutil==5.9.6\\nptyprocess==0.7.0\\npure-eval==0.2.2\\npyarrow==14.0.1\\npyarrow-hotfix==0.6\\npyasn1==0.4.8\\npyasn1-modules==0.2.8\\npycountry==23.12.11\\npycparser==2.21\\npydantic==1.8.2\\npydeck==0.8.1b0\\npygments==2.17.2\\nPyGObject==3.36.0\\npyparsing==3.0.7\\npypdf2==3.0.1\\npyrsistent==0.18.1\\npython-apt==2.0.0+ubuntu0.20.4.6\\npython-dateutil==2.8.2\\npython-docx==1.1.0\\npython-slugify==8.0.1\\npytz==2023.3.post1\\nPyYAML==6.0.1\\npyzmq==22.3.0\\nqtconsole==5.2.2\\nQtPy==2.0.1\\nrank-bm25==0.2.2\\nregex==2023.10.3\\nrequests==2.31.0\\nrequests-oauthlib==1.3.1\\nrequests-unixsocket==0.2.0\\nrich==13.7.0\\nrouge-score==0.1.2\\nrsa==4.8\\nsafetensors==0.4.1\\nscikit-learn==0.24.2\\nscipy==1.10.1\\nseaborn==0.11.2\\nSend2Trash==1.8.0\\nsentence-transformers==2.2.2\\nsentencepiece==0.1.99\\nsix==1.14.0\\nsmart-open==6.4.0\\nsmmap==5.0.1\\nsoupsieve==2.5\\nspacy==3.2.1\\nspacy-legacy==3.0.12\\nspacy-loggers==1.0.5\\nspacy-sentence-bert==0.1.2\\nsrsly==2.4.8\\nstack-data==0.1.4\\nstreamlit==1.29.0\\nSummy==0.1\\nsumy==0.11.0\\ntenacity==8.2.3\\ntensorboard==2.8.0\\ntensorboard-data-server==0.6.1\\ntensorboard-plugin-wit==1.8.1\\ntensorflow==2.8.0\\ntensorflow-io-gcs-filesystem==0.23.1\\ntermcolor==1.1.0\\nterminado==0.13.1\\ntestpath==0.5.0\\ntext-unidecode==1.3\\ntf-estimator-nightly==2.8.0.dev2021122109\\nthinc==8.0.17\\nthreadpoolctl==3.2.0\\ntokenizers==0.15.0\\ntoml==0.10.2\\ntomli==2.0.0\\ntoolz==0.12.0\\ntorch==1.10.0\\ntorchvision==0.11.1\\ntornado==6.1\\ntqdm==4.66.1\\ntraitlets==5.1.1\\ntransformers==4.36.1\\ntyper==0.4.2\\ntyping-extensions==4.9.0\\ntzlocal==5.2\\nurllib3==1.25.8\\nvalidators==0.22.0\\nwasabi==0.10.1\\nwatchdog==3.0.0\\nwcwidth==0.2.5\\nwebencodings==0.5.1\\nWerkzeug==2.0.2\\nwidgetsnbextension==3.5.2\\nwrapt==1.13.3\\nxxhash==3.4.1\\nyarl==1.9.4\\nzipp==3.7.0\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the string\n",
    "\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(\"req.txt\", \"w\") as text_file:\n",
    "    # Write the string to the file\n",
    "    text_file.write(requirements)\n",
    "\n",
    "# Close the file\n",
    "text_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
